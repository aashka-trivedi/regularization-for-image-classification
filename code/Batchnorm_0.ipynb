{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDLS Project\n",
    "\n",
    "# Effects of Regularization Techniques on Image Classification Tasks\n",
    "\n",
    "Dataset: Cifar10\n",
    "\n",
    "Model: ResNet50\n",
    "\n",
    "Batchnorm Layers: 0\n",
    "\n",
    "References:\n",
    "\n",
    "https://towardsdatascience.com/understand-and-implement-resnet-50-with-tensorflow-2-0-1190b9b52691\n",
    "\n",
    "https://github.com/suvoooo/Learn-TensorFlow/blob/master/resnet/Implement_Resnet_TensorFlow.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import  Rectangle\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import tensorflow.keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPool2D,MaxPooling2D, Flatten,BatchNormalization, Dropout,ZeroPadding2D, AveragePooling2D, Add, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import activations\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Model\n",
    "#Removing all regularizers\n",
    "\n",
    "def res_identity(x, filters, num_batchnorm = 0, num_dropout=0, dropout_prob = 0):\n",
    "  #renet block where dimension doesnot change.\n",
    "  #The skip connection is just simple identity conncection\n",
    "  #we will have 3 blocks and then input will be added\n",
    "\n",
    "    x_skip = x # this will be used for addition with the residual block \n",
    "    f1, f2 = filters\n",
    "    bn = num_batchnorm\n",
    "    drp = num_dropout\n",
    "\n",
    "    #first block \n",
    "    x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid')(x)\n",
    "    if bn>0:\n",
    "        x = BatchNormalization()(x)\n",
    "        bn-=1\n",
    "    x = Activation(activations.relu)(x)\n",
    "    if drp>0:\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        drp-=1\n",
    "\n",
    "    #second block # bottleneck (but size kept same with padding)\n",
    "    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    if bn>0:\n",
    "        x = BatchNormalization()(x)\n",
    "        bn-=1\n",
    "    x = Activation(activations.relu)(x)\n",
    "    if drp>0:\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        drp-=1\n",
    "\n",
    "    # third block activation used after adding the input\n",
    "    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid')(x)\n",
    "    if bn>0:\n",
    "        x = BatchNormalization()(x)\n",
    "        bn-=1\n",
    "    x = Activation(activations.relu)(x)\n",
    "    if drp>0:\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        drp-=1\n",
    "\n",
    "    # add the input \n",
    "    x = Add()([x, x_skip])\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "\n",
    "    return x\n",
    "\n",
    "def res_conv(x, s, filters, num_batchnorm = 0, num_dropout=0, dropout_prob = 0):\n",
    "    x_skip = x\n",
    "    f1, f2 = filters\n",
    "    bn = num_batchnorm\n",
    "    drp = num_dropout\n",
    "\n",
    "    # first block\n",
    "    x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid')(x)\n",
    "    # when s = 2 then it is like downsizing the feature map\n",
    "    if bn>0:\n",
    "        x = BatchNormalization()(x)\n",
    "        bn-=1\n",
    "    x = Activation(activations.relu)(x)\n",
    "    if drp>0:\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        drp-=1\n",
    "\n",
    "    # second block\n",
    "    x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    if bn>0:\n",
    "        x = BatchNormalization()(x)\n",
    "        bn-=1\n",
    "    x = Activation(activations.relu)(x)\n",
    "    if drp>0:\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        drp-=1\n",
    "\n",
    "    #third block\n",
    "    x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid')(x)\n",
    "    if bn>0:\n",
    "        x = BatchNormalization()(x)\n",
    "        bn-=1\n",
    "\n",
    "    # shortcut \n",
    "    x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid')(x_skip)\n",
    "    x_skip = BatchNormalization()(x_skip)\n",
    "\n",
    "    # add \n",
    "    x = Add()([x, x_skip])\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def resnet50(num_batchnorm = 0, bn_pooling = False, dropout_prob=0, num_dropout_conv = 0, num_dropout_id = 0, num_dropout=0):\n",
    "\n",
    "    input_im = Input(shape=(32, 32, 3)) # cifar 10 images size\n",
    "    x = ZeroPadding2D(padding=(3, 3))(input_im)\n",
    "    bn = num_batchnorm\n",
    "    drp = num_dropout\n",
    "\n",
    "    # 1st stage\n",
    "    # here we perform maxpooling, see the figure above\n",
    "\n",
    "    x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n",
    "    if bn_pooling:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    if drp>0:\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        drp-=1\n",
    "\n",
    "    #2nd stage \n",
    "    # frm here on only conv block and identity block, no pooling\n",
    "\n",
    "    x = res_conv(x, s=1, filters=(64, 256),num_batchnorm=bn, num_dropout = num_dropout_conv, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(64, 256),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(64, 256),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "\n",
    "    # 3rd stage\n",
    "\n",
    "    x = res_conv(x, s=2, filters=(128, 512),num_batchnorm=bn, num_dropout = num_dropout_conv, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(128, 512),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(128, 512),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(128, 512),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "\n",
    "    # 4th stage\n",
    "\n",
    "    x = res_conv(x, s=2, filters=(256, 1024),num_batchnorm=bn, num_dropout = num_dropout_conv, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(256, 1024),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(256, 1024),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(256, 1024),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(256, 1024),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(256, 1024),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "\n",
    "    # 5th stage\n",
    "\n",
    "    x = res_conv(x, s=2, filters=(512, 2048),num_batchnorm=bn, num_dropout = num_dropout_conv, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(512, 2048),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "    x = res_identity(x, filters=(512, 2048),num_batchnorm=bn, num_dropout = num_dropout_id, dropout_prob = dropout_prob)\n",
    "\n",
    "    # ends with average pooling and dense connection\n",
    "\n",
    "    x = AveragePooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    if drp>0:\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        drp-=1\n",
    "    x = Dense(10, activation='softmax')(x) #multi-class\n",
    "\n",
    "    # define the model \n",
    "\n",
    "    model = Model(inputs=input_im, outputs=x, name='Resnet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to measure Time to Accuracy\n",
    "#https://keras.io/guides/writing_your_own_callbacks/\n",
    "\n",
    "class timeToAccuracy(keras.callbacks.Callback):\n",
    "    def __init__(self, startTime, epoch_ckpt, model_name):\n",
    "        super(timeToAccuracy, self).__init__()\n",
    "        self.targetAcc = 0.87 #CHANGE TO 0.87 WHEN RUNNING MODEL\n",
    "        self.foundTarget = False\n",
    "        self.startTime = startTime\n",
    "        self.epoch = 0\n",
    "        self.epoch_ckpt = epoch_ckpt\n",
    "        self.model_name = model_name\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.epoch % self.epoch_ckpt == 0:\n",
    "            name = self.model_name + '.h5'\n",
    "            self.model.save_weights(name)\n",
    "        self.epoch += 1\n",
    "        \n",
    "        if not self.foundTarget:\n",
    "            if logs['accuracy'] >= self.targetAcc:\n",
    "                current = time.time()\n",
    "                print(\"Time to reach {} accuracy: {} s\".format(self.targetAcc, current-self.startTime))\n",
    "                with open('{}_tta.pkl'.format(model_name), 'wb') as file:\n",
    "                    pickle.dump(current-self.startTime, file)\n",
    "                self.foundTarget = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit, evaluate and checkpoint\n",
    "def fit_resnet(model, xtrain, ytrain, xtest, ytest, model_name):\n",
    "  \n",
    "    EPOCHS = 200      #Change to 200 when running the model\n",
    "    BATCH_SIZE= 64\n",
    "    VERBOSITY = 0   #Change to 0 when actually running model\n",
    "    EPOCH_CKPT = 10 # save model every N epochs\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    model.compile(loss = keras.losses.categorical_crossentropy, optimizer = opt, metrics=['accuracy'])\n",
    "    #fit the model\n",
    "    start = time.time()\n",
    "    model.fit(\n",
    "        xtrain,\n",
    "        ytrain,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=VERBOSITY,\n",
    "        epochs = EPOCHS,\n",
    "        callbacks=[timeToAccuracy(startTime=start, epoch_ckpt=EPOCH_CKPT, model_name=model_name)]\n",
    "    )\n",
    "    train_time = time.time() - start\n",
    "\n",
    "    #evaluate\n",
    "    score = model.evaluate(xtest,ytest)\n",
    "    loss = score[0]\n",
    "    acc = score[1]\n",
    "\n",
    "    return train_time,  acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#prepare data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data() \n",
    "\n",
    "# Data normalization\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "y_train = to_categorical(y_train) \n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_probabilities = [0.2,0.5,0.8]\n",
    "num_dropout = [0,1,2,3]\n",
    "num_batchnorm = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BN=0\n",
    "#loop for all combinations\n",
    "for dp_conv in num_dropout:\n",
    "    for dp_id in num_dropout:\n",
    "        if dp_conv or dp_id:\n",
    "            for prob in dropout_probabilities:\n",
    "                print('Training BN0, DPID{}, DPCONV{}, DPPROB{}'.format(dp_id, dp_conv, prob))\n",
    "                model_name = 'model_bn_0_dpid_{}_dpconv_{}_dpprob_{}.pkl'.format(dp_id, dp_conv, prob)\n",
    "                resnet_model = resnet50(num_batchnorm=0, bn_pooling=True, num_dropout_conv=dp_conv, num_dropout_id=dp_id, dropout_prob=prob)\n",
    "                tt, acc = fit_resnet(resnet_model,x_train, y_train, x_test, y_test, model_name)\n",
    "            \n",
    "                print('Final accuracy {} reached in {}'.format(acc, tt))\n",
    "        else:\n",
    "            prob = 0\n",
    "            print('Training BN0, DPID{}, DPCONV{}, DPPROB{}'.format(dp_id, dp_conv, prob))\n",
    "            model_name = 'model_bn_0_dpid_{}_dpconv_{}_dpprob_{}.pkl'.format(dp_id, dp_conv, prob)\n",
    "            resnet_model = resnet50(num_batchnorm=0, bn_pooling=True, num_dropout_conv=dp_conv, num_dropout_id=dp_id, dropout_prob=prob)\n",
    "            tt, acc = fit_resnet(resnet_model,x_train, y_train, x_test, y_test, model_name)\n",
    "            \n",
    "            print('Final accuracy {} reached in {}'.format(acc, tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BN0, DPID1, DPCONV1, DPPROB0.2\n",
      "Time to reach 0.87 accuracy: 416.76601004600525 s\n",
      "313/313 [==============================] - 4s 10ms/step - loss: 1.3709 - accuracy: 0.7403\n",
      "Final accuracy 0.7402999997138977 reached in 4164.198269844055\n",
      "Training BN0, DPID1, DPCONV1, DPPROB0.5\n",
      "Time to reach 0.87 accuracy: 886.6885948181152 s\n",
      "313/313 [==============================] - 4s 10ms/step - loss: 1.5099 - accuracy: 0.7122\n",
      "Final accuracy 0.7121999859809875 reached in 4139.772803068161\n",
      "Training BN0, DPID1, DPCONV1, DPPROB0.8\n",
      "Time to reach 0.87 accuracy: 1318.508838891983 s\n",
      "313/313 [==============================] - 4s 10ms/step - loss: 1.9497 - accuracy: 0.6577\n",
      "Final accuracy 0.6577000021934509 reached in 4186.440722465515\n",
      "Training BN0, DPID2, DPCONV2, DPPROB0.2\n",
      "Time to reach 0.87 accuracy: 920.297521352768 s\n",
      "313/313 [==============================] - 4s 10ms/step - loss: 1.2565 - accuracy: 0.7230\n",
      "Final accuracy 0.7229999899864197 reached in 4373.524525642395\n",
      "Training BN0, DPID2, DPCONV2, DPPROB0.5\n",
      "Time to reach 0.87 accuracy: 2232.498969078064 s\n",
      "313/313 [==============================] - 4s 10ms/step - loss: 1.5621 - accuracy: 0.6532\n",
      "Final accuracy 0.6531999707221985 reached in 4374.6323227882385\n",
      "Training BN0, DPID2, DPCONV2, DPPROB0.8\n",
      "313/313 [==============================] - 4s 10ms/step - loss: 1.5717 - accuracy: 0.5403\n",
      "Final accuracy 0.5403000116348267 reached in 4401.841610193253\n",
      "Training BN0, DPID3, DPCONV3, DPPROB0.2\n",
      "Time to reach 0.87 accuracy: 1371.5283069610596 s\n",
      "313/313 [==============================] - 4s 10ms/step - loss: 1.3586 - accuracy: 0.7178\n",
      "Final accuracy 0.7178000211715698 reached in 4431.205626487732\n",
      "Training BN0, DPID3, DPCONV3, DPPROB0.5\n",
      "Time to reach 0.87 accuracy: 3248.3489105701447 s\n",
      "313/313 [==============================] - 4s 10ms/step - loss: 1.3148 - accuracy: 0.6530\n",
      "Final accuracy 0.652999997138977 reached in 4454.618563175201\n",
      "Training BN0, DPID3, DPCONV3, DPPROB0.8\n",
      "Time to reach 0.87 accuracy: 4176.732324123383 s\n",
      "313/313 [==============================] - 4s 10ms/step - loss: 2.4699 - accuracy: 0.4785\n",
      "Final accuracy 0.47850000858306885 reached in 4516.135431528091\n"
     ]
    }
   ],
   "source": [
    "#BN=0\n",
    "#DropCONV = DropID\n",
    "num_dropout = [1,2,3]\n",
    "for dp in num_dropout:\n",
    "    if dp:\n",
    "        for prob in dropout_probabilities:\n",
    "            print('Training BN0, DPID{}, DPCONV{}, DPPROB{}'.format(dp, dp, prob))\n",
    "            model_name = 'model_bn_0_dpid_{}_dpconv_{}_dpprob_{}.pkl'.format(dp, dp, prob)\n",
    "            resnet_model = resnet50(num_batchnorm=0, bn_pooling=True, num_dropout_conv=dp, num_dropout_id=dp, dropout_prob=prob)\n",
    "            tt, acc = fit_resnet(resnet_model,x_train, y_train, x_test, y_test, model_name)\n",
    "            \n",
    "            print('Final accuracy {} reached in {}'.format(acc, tt))\n",
    "    else:\n",
    "        prob = 0\n",
    "        print('Training BN0, DPID{}, DPCONV{}, DPPROB{}'.format(dp, dp_conv, prob))\n",
    "        model_name = 'model_bn_0_dpid_{}_dpconv_{}_dpprob_{}.pkl'.format(dp, dp, prob)\n",
    "        resnet_model = resnet50(num_batchnorm=0, bn_pooling=True, num_dropout_conv=dp, num_dropout_id=dp, dropout_prob=prob)\n",
    "        tt, acc = fit_resnet(resnet_model,x_train, y_train, x_test, y_test, model_name)\n",
    "            \n",
    "        print('Final accuracy {} reached in {}'.format(acc, tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
